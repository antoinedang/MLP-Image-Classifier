{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "yUZTrgBrTf0J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u1FWzua7SBWX"
      },
      "outputs": [],
      "source": [
        "def unpickle(file):\n",
        "    with open(file, 'rb') as fo:\n",
        "        dict = pickle.load(fo, encoding='bytes')\n",
        "    return dict"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
        "!mkdir /content/cifar10\n",
        "!tar -xvf  '/content/cifar-10-python.tar.gz' -C '/content/cifar10'"
      ],
      "metadata": {
        "id": "DMYjXxxRmSj6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_batches = []\n",
        "dir = \"/content/cifar10/cifar-10-batches-py/\"\n",
        "\n",
        "for i in range(1,6):\n",
        "  data_batches += [unpickle(dir+\"data_batch_\"+str(i))]\n",
        "  data_batches[i-1][b'labels'] = np.reshape(data_batches[i-1][b'labels'],(len(data_batches[i-1][b'labels']),1))\n",
        "test_batch = unpickle(dir+\"test_batch\")\n",
        "meta_info = unpickle(dir+\"batches.meta\")"
      ],
      "metadata": {
        "id": "l5DbhM90Tcf9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_keys = list(data_batches[0].keys())\n",
        "print(batch_keys)"
      ],
      "metadata": {
        "id": "h8gZfeoxiBNR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data_batches[0][b'data'])"
      ],
      "metadata": {
        "id": "Lkq62GbRpTpo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.max(data_batches[0][b'labels']))"
      ],
      "metadata": {
        "id": "u6nuC7VXpkri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data_batches[0][b'data'].shape)\n",
        "print(data_batches[0][b'labels'].shape)"
      ],
      "metadata": {
        "id": "UZ0M5KyQ_fZa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data_batches)"
      ],
      "metadata": {
        "id": "uSRuSXbZp0YH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(meta_info)"
      ],
      "metadata": {
        "id": "75nvzn3WC7Fq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def logistic(x): return np.ones(x.shape) / (np.exp(-x)+1)\n",
        "\n",
        "def logistic_gradient(x): return (np.ones(x.shape)-logistic(x)) * logistic(x)\n",
        "\n",
        "def hyperbolic_tan(x): return np.tanh(x)\n",
        "\n",
        "def hyperbolic_tan_gradient(x): return np.square(np.ones(x.shape) / np.cosh(x))\n",
        "\n",
        "def relu(x): return np.maximum(np.zeros(x.shape), x)\n",
        "\n",
        "def relu_gradient(x): return 1.0 * (x > 0)\n",
        "\n",
        "def leaky_relu(x): return np.maximum(np.zeros(x.shape), x) + 0.01*np.minimum(np.zeros(x.shape), x)\n",
        "\n",
        "def leaky_relu_gradient(x):  return 1.0 * (x > 0) + 0.01 * (x <= 0)\n",
        "\n",
        "def softplus(x): return np.log(np.ones(x.shape) + np.exp(x))\n",
        "\n",
        "def softplus_gradient(x): return logistic(x)\n",
        "\n",
        "def cross_entropy_loss(y, yh):\n",
        "    return -(y * np.log(yh)) - ((np.ones(y.shape)-y)*(np.log(np.ones(yh.shape)-yh)))\n",
        "\n",
        "def cross_entropy_loss_gradient(y, yh):\n",
        "        summand1 = y / yh\n",
        "        summand2 = (np.ones(y.shape)-y) / (np.ones(yh.shape)-yh)\n",
        "        return summand1 + summand2\n",
        "\n",
        "def softmax(self, yh):\n",
        "    yh_out = np.array(yh.shape)\n",
        "    for i in range(len(yh)):\n",
        "        denominator = np.sum(np.exp(yh[i]))\n",
        "        for j in range(len(yh[i])):\n",
        "            yh_out[i][j] = math.exp(yh[i][j]) / denominator\n",
        "    return yh_out\n",
        "\n",
        "def softmax_gradient(yh):\n",
        "    return yh * (np.ones(yh.shape)-yh)\n",
        "\n",
        "def evaluate_acc(y, yh):\n",
        "    correct = 0\n",
        "    false = 0\n",
        "    for i in range(len(y)):\n",
        "        true = np.argmax(y[i])\n",
        "        pred = np.argmax(yh[i])\n",
        "        if true == pred: correct += 1\n",
        "        else: false += 1\n",
        "    return correct / (false + correct)"
      ],
      "metadata": {
        "id": "wGLwV8Nk8TkP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GradientDescent:\n",
        "    def __init__(self, learning_rate=.001, max_iters=1e4, epsilon=1e-8, momentum=0, batch_size=None):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.max_iters = max_iters\n",
        "        self.epsilon = epsilon\n",
        "        self.momentum = momentum\n",
        "        self.previousGrad = None\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def make_batches(self, x, y, sizeOfMiniBatch):\n",
        "        if (sizeOfMiniBatch==None):\n",
        "            return [(x,y)]\n",
        "        if x.ndim == 1:\n",
        "            x = x[:, None]                      #add a dimension for the features\n",
        "        batches = []\n",
        "        x_length = len(x[0])\n",
        "        datax = pd.DataFrame(x)\n",
        "        datay = pd.DataFrame(y)\n",
        "        data = pd.concat([datax,datay],axis=1, join='inner')\n",
        "        #data = data.sample(frac=1, random_state=1).reset_index(drop=True)\n",
        "        x = data.iloc[:,:x_length]\n",
        "        y = data.iloc[:,x_length:]\n",
        "        numberOfRowsData = x.shape[0]        #number of rows in our data\n",
        "        i = 0\n",
        "        for i in range(int(numberOfRowsData/sizeOfMiniBatch)):\n",
        "            endOfBatch= (i+1)*sizeOfMiniBatch           \n",
        "            if endOfBatch<numberOfRowsData: #if end of the batch is still within range allowed\n",
        "                single_batch_x = x.iloc[i * sizeOfMiniBatch:endOfBatch, :] #slice into a batch\n",
        "                single_batch_y = y.iloc[i * sizeOfMiniBatch:endOfBatch, :] #slice into a batch\n",
        "                batches.append((single_batch_x, single_batch_y))\n",
        "            else: #if end of batch not within range \n",
        "                single_batch_x = x.iloc[i * sizeOfMiniBatch:numberOfRowsData, :] #slice into a batch\n",
        "                single_batch_y = y.iloc[i * sizeOfMiniBatch:numberOfRowsData, :] #slice into a batch\n",
        "                batches.append((single_batch_x, single_batch_y))\n",
        "        return batches\n",
        "            \n",
        "    def run(self, gradient_fn, x, y, params, test_x, test_y, model):\n",
        "        batches = self.make_batches(x,y, self.batch_size)\n",
        "        norms = np.array([np.inf])\n",
        "        t = 1\n",
        "        epoch = 1\n",
        "        i = 1\n",
        "        while np.any(norms > self.epsilon) and i < self.max_iters:\n",
        "            if (t-1)>=len(batches):\n",
        "                #new epoch\n",
        "                #evaluate model performance every epoch (for plotting and stuff)\n",
        "                model.params = params\n",
        "                print(\"epoch\", epoch, \"completed. Train accuracy:\", evaluate_acc(y, model.predict(x)), \". Test accuracy:\", evaluate_acc(test_y, model.predict(test_x)))\n",
        "                epoch += 1\n",
        "                batches = self.make_batches(x,y, self.batch_size)\n",
        "                t=1\n",
        "            grad = gradient_fn(batches[t-1][0], batches[t-1][1], params)\n",
        "            if self.previousGrad is None: self.previousGrad = grad\n",
        "            grad = [grad[i]*(1.0-self.momentum) + self.previousGrad[i]*self.momentum for i in range(len(grad))]\n",
        "            self.previousGrad = grad\n",
        "            for p in range(len(params)):\n",
        "                params[p] -= self.learning_rate * grad[p]\n",
        "            t += 1\n",
        "            i += 1\n",
        "            norms = np.array([np.linalg.norm(g) for g in grad])\n",
        "        self.iterationsPerformed = i\n",
        "        return params\n",
        "\n",
        "class MLP:\n",
        "    def __init__(self, activation, activation_gradient, nonlinearity, nonlinearity_gradient, loss_gradient, hidden_layers=2, hidden_units=[64, 64], min_init_weight=0, dropout_p=0):\n",
        "        if (hidden_layers != len(hidden_units)):\n",
        "            print(\"Must have same number of hidden unit sizes as hidden layers!\")\n",
        "            exit()\n",
        "        self.hidden_layers = hidden_layers\n",
        "        self.hidden_units = hidden_units\n",
        "        self.activation = activation\n",
        "        self.activation_gradient = activation_gradient\n",
        "        self.min_init_weight = min_init_weight\n",
        "        self.nonlinearity = nonlinearity\n",
        "        self.nonlinearity_gradient = nonlinearity_gradient\n",
        "        self.loss_gradient = loss_gradient\n",
        "        self.dropout_p = dropout_p\n",
        "            \n",
        "    def fit(self, x, y, optimizer, test_x, test_y):\n",
        "        N,D = x.shape\n",
        "        _,C = y.shape\n",
        "        weight_shapes = [D]\n",
        "        weight_shapes.extend([m for m in self.hidden_units])\n",
        "        weight_shapes.append(C)\n",
        "        params_init = []\n",
        "        for i in range(len(weight_shapes)-1):\n",
        "            w = np.random.randn(weight_shapes[i], weight_shapes[i+1]) * .01\n",
        "            w += np.ones((weight_shapes[i], weight_shapes[i+1]))*(self.min_init_weight-np.min(w))\n",
        "            params_init.append(w)\n",
        "        self.params = optimizer.run(self.activation_gradient, x, y, params_init, self, test_x, test_y)\n",
        "        return self\n",
        "\n",
        "    #NOT SURE WHAT TO DO HERE --------------------------------------------\n",
        "    def gradient(x, y, params):\n",
        "        yh = x\n",
        "        steps = []\n",
        "        dropout_layers = []\n",
        "        for w in params:\n",
        "            not_dropped = (np.random.randn(yh.shape) > self.dropout_p) * 1.0\n",
        "            dropout_layers.append(not_dropped)\n",
        "            if w != params[-1]: yh = self.activation(np.dot(yh, w*not_dropped))\n",
        "            else: yh = np.dot(yh, w*not_dropped)\n",
        "            steps.append(yh)\n",
        "        yh = self.nonlinearity(yh)\n",
        "        gradient = self.loss_gradient(y, yh) #NxC\n",
        "        gradient = np.dot(gradient, self.nonlinearity_gradient(gradient))\n",
        "        #backpropagation\n",
        "        gradients = [gradient]\n",
        "        for w in params(::-1):\n",
        "            #only add activation gradient if not on the last weights (last weights go straight to softmax)\n",
        "            if w != params[-1]: dw = self.activation_gradient()\n",
        "            else: dw = w\n",
        "            gradient = np.dot(gradient, dw)\n",
        "            gradients = list([gradient]).extend(gradients)\n",
        "        return gradients\n",
        "    #PRETTY SURE THATS NOT RIGHT --------------------------------------------------------\n",
        "    \n",
        "    def predict(self, x):\n",
        "        yh = x\n",
        "        for w in self.params:\n",
        "            #dropout w/ weight scaling\n",
        "            w *= (1.0-self.dropout_p)\n",
        "            #don't do activation function on last weights\n",
        "            if w != params[-1]: yh = self.activation(np.dot(yh, w))\n",
        "            else: yh = np.dot(yh, w)\n",
        "        return self.nonlinearity(yh)"
      ],
      "metadata": {
        "id": "vci9KwvLlKu0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}