{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!nvcc --version\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "QrsNXAumlnFK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r sample_data\n",
        "!wget http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
        "!mkdir /content/cifar10\n",
        "!tar -xvf  '/content/cifar-10-python.tar.gz' -C '/content/cifar10'\n",
        "!rm cifar-10-python.tar.gz*"
      ],
      "metadata": {
        "id": "9MjO-6SSgbeG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "NMfxuG_KMzBY"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "#import numpy as np\n",
        "import cupy as np\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from numba import jit, cuda\n",
        "\n",
        "def unpickle(file):\n",
        "    with open(file, 'rb') as fo:\n",
        "        dict = pickle.load(fo, encoding='bytes')\n",
        "    return dict\n",
        "\n",
        "def logistic(x): return np.ones(x.shape) / (np.exp(-x)+1)\n",
        "\n",
        "def logistic_gradient(x): return (np.ones(x.shape)-logistic(x)) * logistic(x)\n",
        "\n",
        "def hyperbolic_tan(x): return np.tanh(x)\n",
        "\n",
        "def hyperbolic_tan_gradient(x): return np.square(np.ones(x.shape) / np.cosh(x))\n",
        "\n",
        "def relu(x): return np.maximum(np.zeros(x.shape), x)\n",
        "\n",
        "def relu_gradient(x): return 1.0 * (x > 0)\n",
        "\n",
        "def leaky_relu(x): return np.maximum(np.zeros(x.shape), x) + 0.01*np.minimum(np.zeros(x.shape), x)\n",
        "\n",
        "def leaky_relu_gradient(x):  return 1.0 * (x > 0) + 0.01 * (x <= 0)\n",
        "\n",
        "def softplus(x): return np.log(np.ones(x.shape) + np.exp(x))\n",
        "\n",
        "def softplus_gradient(x): return logistic(x)\n",
        "\n",
        "def softmax(yh):\n",
        "    denom = np.sum(np.exp(yh-np.max(yh)), axis=1, keepdims=True)\n",
        "    return np.exp(yh-np.max(yh))/denom\n",
        "\n",
        "def evaluate_acc(y, yh):\n",
        "    return 0\n",
        "    correct = 0\n",
        "    false = 0\n",
        "    for i in range(len(y)):\n",
        "        true = np.argmax(y[i])\n",
        "        pred = np.argmax(yh[i])\n",
        "        if true == pred: correct += 1\n",
        "        else: false += 1\n",
        "    return correct / (false + correct)\n",
        "\n",
        "def add_bias(feat):\n",
        "    return np.append(np.ones((feat.shape[0],1)),feat,axis=1)\n",
        "\n",
        "def add_diffd_bias(feat):\n",
        "    return np.append(np.zeros((feat.shape[0],1)),feat,axis=1)\n",
        "\n",
        "class GradientDescent:\n",
        "    def __init__(self, learning_rate=.01, max_iters=1e4, epsilon=1e-8, momentum=0, batch_size=None):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.max_iters = max_iters\n",
        "        self.epsilon = epsilon\n",
        "        self.momentum = momentum\n",
        "        self.previousGrad = None\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def make_batches(self, x, y, sizeOfMiniBatch):\n",
        "        if (sizeOfMiniBatch==None):\n",
        "            return [x,y]\n",
        "        if x.ndim == 1:\n",
        "            x = x[:, None]                      #add a dimension for the features\n",
        "        batches = []\n",
        "        x_length = len(x[0])\n",
        "        datax = pd.DataFrame(x)\n",
        "        datay = pd.DataFrame(y)\n",
        "        data = pd.concat([datax,datay],axis=1, join='inner')\n",
        "        #data = data.sample(frac=1, random_state=1).reset_index(drop=True)\n",
        "        x = data.iloc[:,:x_length]\n",
        "        y = data.iloc[:,x_length:]\n",
        "        numberOfRowsData = x.shape[0]        #number of rows in our data\n",
        "        i = 0\n",
        "        for i in range(int(numberOfRowsData/sizeOfMiniBatch)):\n",
        "            endOfBatch= (i+1)*sizeOfMiniBatch           \n",
        "            if endOfBatch<numberOfRowsData: #if end of the batch is still within range allowed\n",
        "                single_batch_x = x.iloc[i * sizeOfMiniBatch:endOfBatch, :] #slice into a batch\n",
        "                single_batch_y = y.iloc[i * sizeOfMiniBatch:endOfBatch, :] #slice into a batch\n",
        "                batches.append((single_batch_x, single_batch_y))\n",
        "            else: #if end of batch not within range \n",
        "                single_batch_x = x.iloc[i * sizeOfMiniBatch:numberOfRowsData, :] #slice into a batch\n",
        "                single_batch_y = y.iloc[i * sizeOfMiniBatch:numberOfRowsData, :] #slice into a batch\n",
        "                batches.append((single_batch_x, single_batch_y))\n",
        "        return batches\n",
        "    \n",
        "    def run(self, gradient_fn, x, y, params, test_x, test_y, model):\n",
        "        batches = self.make_batches(x,y, self.batch_size)\n",
        "        norms = np.array([np.inf])\n",
        "        t = 1\n",
        "        epoch = 1\n",
        "        i = 1\n",
        "        while np.any(norms > self.epsilon) and i < self.max_iters:\n",
        "            if (t-1)>=len(batches):\n",
        "                #new epoch\n",
        "                #evaluate model performance every epoch (for plotting and stuff)\n",
        "                model.params = params\n",
        "                print(\"epoch\", epoch, \"completed. Train accuracy:\", evaluate_acc(y, model.predict(x)), \". Test accuracy:\", evaluate_acc(test_y, model.predict(test_x)))\n",
        "                epoch += 1\n",
        "                batches = self.make_batches(x,y, self.batch_size)\n",
        "                t=1\n",
        "            grad = gradient_fn(batches[0], batches[1], params)\n",
        "            if self.previousGrad is None: self.previousGrad = grad\n",
        "            grad = [grad[i]*(1.0-self.momentum) + self.previousGrad[i]*self.momentum for i in range(len(grad))]\n",
        "            self.previousGrad = grad\n",
        "            for p in range(len(params)):\n",
        "                params[p] -= self.learning_rate * grad[p]\n",
        "            t += 1\n",
        "            i += 1\n",
        "            norms = np.array([np.linalg.norm(g) for g in grad])\n",
        "        self.iterationsPerformed = i\n",
        "        model.params = params\n",
        "        print(\"epoch\", epoch, \"completed. Train accuracy:\", evaluate_acc(y, model.predict(x)), \". Test accuracy:\", evaluate_acc(test_y, model.predict(test_x)))\n",
        "        return params\n",
        "\n",
        "class MLP:\n",
        "    def __init__(self, activation, activation_gradient, hidden_layers=2, hidden_units=[64, 64], dropout_p=0):\n",
        "        if (hidden_layers != len(hidden_units)):\n",
        "            print(\"Must have same number of hidden unit sizes as hidden layers!\")\n",
        "            exit()\n",
        "        self.hidden_layers = hidden_layers\n",
        "        self.hidden_units = hidden_units\n",
        "        self.activation = activation\n",
        "        self.activation_gradient = activation_gradient\n",
        "        self.dropout_p = dropout_p\n",
        "            \n",
        "    def init_params(self, x, y):\n",
        "        N,D = x.shape\n",
        "        _,C = y.shape\n",
        "        weight_shapes = [D]\n",
        "        weight_shapes.extend([m for m in self.hidden_units])\n",
        "        weight_shapes.append(C)\n",
        "        params_init = []\n",
        "        for i in range(len(weight_shapes)-1):\n",
        "            w = np.random.randn(weight_shapes[i]+1, weight_shapes[i+1]) * .01\n",
        "            #w += np.ones((weight_shapes[i]+1, weight_shapes[i+1]))*(self.min_init_weight+abs(np.min(w)))\n",
        "            params_init.append(w)\n",
        "        return params_init\n",
        "\n",
        "    def fit(self, x, y, optimizer, test_x, test_y):\n",
        "        params_init = self.init_params(x, y)\n",
        "        self.params = optimizer.run(self.gradient, x, y, params_init, test_x, test_y, self)\n",
        "        return self\n",
        "\n",
        "    def gradient(self, x, y, params):\n",
        "        W_l = params[0]\n",
        "        N,D = x.shape\n",
        "        z_l = x\n",
        "        z_l_biased = add_bias(z_l)\n",
        "        a_l = np.dot(z_l_biased,W_l)\n",
        "        a = [a_l]\n",
        "          \n",
        "        for l in range(1, self.hidden_layers):\n",
        "            W_l = params[l]\n",
        "            z_l = self.activation(a_l)\n",
        "            z_l_biased = add_bias(z_l)\n",
        "            a_l = np.dot(z_l_biased,W_l)\n",
        "            a += [a_l]\n",
        "\n",
        "        W_l = params[-1]\n",
        "        z_l = self.activation(a_l)\n",
        "        z_l_biased = add_bias(z_l)\n",
        "        a_l = np.dot(z_l_biased,W_l)\n",
        "        yh = softmax(a_l)\n",
        "            \n",
        "        gradient = yh-y\n",
        "        dparams = [np.dot(add_bias(self.activation(a[-1])).T, gradient)/N]\n",
        "\n",
        "        for l in range(self.hidden_layers-1,0,-1):\n",
        "            gradient = self.activation_gradient(a[l])*np.dot(gradient, params[l+1][1:,:].T)\n",
        "            dparams.insert(0, np.dot(add_bias(self.activation(a[l-1])).T, gradient)/N)\n",
        "        \n",
        "        gradient = self.activation_gradient(a[0])*np.dot(gradient, params[1][1:,:].T)\n",
        "        dparams.insert(0, np.dot(add_bias(x).T, gradient)/N)\n",
        "\n",
        "        return dparams\n",
        "    \n",
        "    def predict(self, x):\n",
        "        yh = x\n",
        "        for i in range(len(self.params)):\n",
        "            w = self.params[i]\n",
        "            #dropout w/ weight scaling\n",
        "            w *= (1.0-self.dropout_p)\n",
        "            #don't do activation function on last weights\n",
        "            yh = add_bias(yh)\n",
        "            if i != len(self.params) - 1: yh = self.activation(np.dot(yh, w))\n",
        "            else: yh = softmax(np.dot(yh, w))\n",
        "        return yh\n",
        "\n",
        "def getData():\n",
        "    data_batches = []\n",
        "    directory = \"/content/cifar10/cifar-10-batches-py/\"\n",
        "\n",
        "    train_x = None\n",
        "    train_y = None\n",
        "\n",
        "    for i in range(1,6):\n",
        "        new_batch = unpickle(directory+\"data_batch_\"+str(i))\n",
        "        if train_x is None:\n",
        "            train_x = new_batch[b'data']\n",
        "            train_y = np.reshape(np.array(new_batch[b'labels']), (10000,1))\n",
        "        else:\n",
        "            train_x = np.row_stack([train_x, new_batch[b'data']])\n",
        "            train_y = np.row_stack([train_y, np.reshape(np.array(new_batch[b'labels']), (10000,1))])\n",
        "\n",
        "    test_batch = unpickle(directory+\"test_batch\")\n",
        "    test_x = test_batch[b'data']\n",
        "    test_y = test_batch[b'labels']\n",
        "\n",
        "    new_train_y = np.zeros((len(train_y), 10))\n",
        "    new_test_y = np.zeros((len(test_y), 10))\n",
        "\n",
        "    #one hot encoding labels\n",
        "    for i in range(len(train_y)):\n",
        "        new_train_y[i][train_y[i]] = 1\n",
        "    train_y = new_train_y\n",
        "\n",
        "    for i in range(len(test_y)):\n",
        "        new_test_y[i][test_y[i]] = 1\n",
        "    test_y = new_test_y\n",
        "\n",
        "    \n",
        "    #normalizing the images for each batch\n",
        "    #division by the magnitude to improve convergence speed of gradient descent \n",
        "    train_x = np.array(train_x, dtype=np.float64)\n",
        "    test_x = np.array(test_x, dtype=np.float64)\n",
        "    train_x *= 1/255\n",
        "    test_x *= 1/255\n",
        "\n",
        "    return train_x, train_y, test_x, test_y"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_x, train_y, test_x, test_y = getData()\n",
        "\n",
        "print(\"Loaded data successfully.\")\n",
        "\n",
        "optimizer = GradientDescent(learning_rate=.1, max_iters=1e4, epsilon=1e-8, momentum=0, batch_size=None)\n",
        "model = MLP(activation=relu, activation_gradient=relu_gradient, hidden_layers=2, hidden_units=[64, 64], dropout_p=0)\n",
        "\n",
        "model.fit(train_x, train_y, optimizer, test_x, test_x)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450
        },
        "id": "Ips-XYnsgKUe",
        "outputId": "792afee6-c0ed-44b9-8218-0afecd586744"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 25 completed. Train accuracy: 0 . Test accuracy: 0\n",
            "epoch 26 completed. Train accuracy: 0 . Test accuracy: 0\n",
            "epoch 27 completed. Train accuracy: 0 . Test accuracy: 0\n",
            "epoch 28 completed. Train accuracy: 0 . Test accuracy: 0\n",
            "epoch 29 completed. Train accuracy: 0 . Test accuracy: 0\n",
            "epoch 30 completed. Train accuracy: 0 . Test accuracy: 0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-ac20ec386ed2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMLP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation_gradient\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrelu_gradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_units\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout_p\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-19-bdabc1b7c236>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, optimizer, test_x, test_y)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0mparams_init\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams_init\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-bdabc1b7c236>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, gradient_fn, x, y, params, test_x, test_y, model)\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0mepoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m         \u001b[0;32mwhile\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorms\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_iters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m                 \u001b[0;31m#new epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}