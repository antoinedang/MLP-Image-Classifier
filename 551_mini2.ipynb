{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97PVQi2YAkcK"
      },
      "source": [
        "# Download, unzip, normalize data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9MjO-6SSgbeG"
      },
      "outputs": [],
      "source": [
        "!rm -r sample_data\n",
        "!wget http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
        "!mkdir /content/cifar10\n",
        "!tar -xvf  '/content/cifar-10-python.tar.gz' -C '/content/cifar10'\n",
        "!rm cifar-10-python.tar.gz*"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup MLP and Gradient Descent"
      ],
      "metadata": {
        "id": "MFrt75oskg1q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NMfxuG_KMzBY"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "#import numpy as np\n",
        "import cupy as np\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "from tabulate import tabulate\n",
        "import torch\n",
        "from torch import nn\n",
        "from torchvision import models\n",
        "from torchvision.models import ResNet18_Weights\n",
        "\n",
        "def unpickle(file):\n",
        "    with open(file, 'rb') as fo:\n",
        "        dict = pickle.load(fo, encoding='bytes')\n",
        "    return dict\n",
        "\n",
        "def logistic(x): return np.ones(x.shape) / (np.exp(-x)+1)\n",
        "\n",
        "def logistic_gradient(x): return (np.ones(x.shape)-logistic(x)) * logistic(x)\n",
        "\n",
        "def hyperbolic_tan(x): return np.tanh(x)\n",
        "\n",
        "def hyperbolic_tan_gradient(x): return np.square(np.ones(x.shape) / np.cosh(x))\n",
        "\n",
        "def relu(x): return np.maximum(np.zeros(x.shape), x)\n",
        "\n",
        "def relu_gradient(x): return 1.0 * (x > 0)\n",
        "\n",
        "def leaky_relu(x): return np.maximum(np.zeros(x.shape), x) + 0.01*np.minimum(np.zeros(x.shape), x)\n",
        "\n",
        "def leaky_relu_gradient(x):  return 1.0 * (x > 0) + 0.01 * (x <= 0)\n",
        "\n",
        "def softplus(x): return np.log(np.ones(x.shape) + np.exp(x))\n",
        "\n",
        "def softplus_gradient(x): return logistic(x)\n",
        "\n",
        "def softmax(yh):\n",
        "    denom = np.sum(np.exp(yh - np.max(yh)), axis=1, keepdims=True)\n",
        "    return np.exp(yh-np.max(yh))/denom\n",
        "\n",
        "def evaluate_acc(y, yh):\n",
        "    correct = 0\n",
        "    false = 0\n",
        "    true = np.argmax(y, axis=1)\n",
        "    pred = np.argmax(yh, axis=1)\n",
        "    diff = true - pred\n",
        "    false = np.count_nonzero(diff)\n",
        "    correct = len(y)-false\n",
        "    return float(correct / (false + correct))\n",
        "\n",
        "def add_bias(feat):\n",
        "    return np.append(np.ones((feat.shape[0],1)),feat,axis=1)\n",
        "\n",
        "def add_diffd_bias(feat):\n",
        "    return np.append(np.zeros((feat.shape[0],1)),feat,axis=1)\n",
        "\n",
        "class GradientDescent:\n",
        "    def __init__(self, learning_rate=.01, max_iters=np.inf, epsilon=1e-8, momentum=0, batch_size=None, l1_strength=0, l2_strength=0, after_epochs_stop=100, printAccuracy=True, maxEpochs=300):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.max_iters = max_iters\n",
        "        self.epsilon = epsilon\n",
        "        self.momentum = momentum\n",
        "        self.previousGrad = None\n",
        "        self.batch_size = batch_size\n",
        "        self.test_accuracy_per_epoch = []\n",
        "        self.train_accuracy_per_epoch = []\n",
        "        self.l1_strength = l1_strength\n",
        "        self.l2_strength = l2_strength\n",
        "        self.after_epochs_stop = after_epochs_stop\n",
        "        self.printAccuracy= printAccuracy\n",
        "        self.maxEpochs = maxEpochs\n",
        "\n",
        "    def make_batches(self, x, y, sizeOfMiniBatch):\n",
        "        if (sizeOfMiniBatch==None):\n",
        "            return [(x,y)]\n",
        "        batches = []\n",
        "        x_length = len(x)\n",
        "        i = 0\n",
        "        for i in range(int(x_length/sizeOfMiniBatch)):\n",
        "            endOfBatch= (i+1)*sizeOfMiniBatch           \n",
        "            if endOfBatch<x_length: #if end of the batch is still within range allowed\n",
        "                single_batch_x = x[i * sizeOfMiniBatch: endOfBatch, :]#slice into a batch\n",
        "                single_batch_y = y[i * sizeOfMiniBatch: endOfBatch, :] #slice into a batch\n",
        "                batches.append((single_batch_x, single_batch_y))\n",
        "            else: #if end of batch not within range \n",
        "                single_batch_x = x[i * sizeOfMiniBatch:x_length, :] #slice into a batch\n",
        "                single_batch_y = y[i * sizeOfMiniBatch:x_length, :] #slice into a batch\n",
        "                batches.append((single_batch_x, single_batch_y))\n",
        "        return batches\n",
        "    \n",
        "    def run(self, gradient_fn, x, y, params, test_x, test_y, model):\n",
        "        self.previousGrad = None\n",
        "        earlyStop = 0\n",
        "        self.max_accuracy = 0.0\n",
        "        batches = self.make_batches(x,y, self.batch_size)\n",
        "        norms = np.array([np.inf])\n",
        "        t = 1\n",
        "        epoch = 1\n",
        "        i = 1\n",
        "        while np.any(norms > self.epsilon) and i < self.max_iters:\n",
        "            if (t-1)>=len(batches):\n",
        "                #new epoch\n",
        "                #evaluate model performance every epoch (for plotting and stuff)\n",
        "                model.params = params\n",
        "                test_acc = evaluate_acc(test_y, model.predict(test_x))\n",
        "                train_acc = evaluate_acc(y, model.predict(x))\n",
        "                self.test_accuracy_per_epoch.append(test_acc)\n",
        "                self.train_accuracy_per_epoch.append(train_acc)\n",
        "\n",
        "                if (self.printAccuracy):\n",
        "                  print(\"epoch\", epoch, \"completed. Train accuracy:\", train_acc, \". Test accuracy:\", test_acc)\n",
        "\n",
        "                #for ealy stoppage, increment every time accuracy is below previous max\n",
        "                if (self.max_accuracy > test_acc):\n",
        "                  earlyStop += 1\n",
        "                else:\n",
        "                    earlyStop = 0\n",
        "                if (earlyStop >= self.after_epochs_stop): #for 100 epochs in a row, no important change in accuracy\n",
        "                  print(\"Early Stoppage, test accuracy has not improved in 100 epochs\")\n",
        "                  break\n",
        "                self.max_accuracy = max(test_acc, self.max_accuracy)\n",
        "                epoch += 1\n",
        "                batches = self.make_batches(x,y, self.batch_size)\n",
        "                t=1\n",
        "\n",
        "            if (epoch > self.maxEpochs): \n",
        "              print(\"Reached max epochs\")\n",
        "              break\n",
        "              \n",
        "            grad = gradient_fn(batches[t-1][0], batches[t-1][1], params)\n",
        "            #add momentum\n",
        "            if self.previousGrad is None: self.previousGrad = grad\n",
        "            grad = [grad[i]*(1.0-self.momentum) + self.previousGrad[i]*self.momentum for i in range(len(grad))]\n",
        "            self.previousGrad = grad\n",
        "            #add regularization\n",
        "            for p in range(len(params)):\n",
        "                #print(\"weights\", params[p])\n",
        "                #print(\"gradient before L1/L2\", grad[p])\n",
        "                #print(\"L1 penalty\", self.weight_scale*self.l1_strength*np.sign(params[p]))\n",
        "                #print(\"L2 penalty\", self.weight_scale*((1/2)*params[p]*self.l2_strength))\n",
        "                grad[p] = np.add(grad[p], self.l1_strength*np.sign(params[p]))\n",
        "                grad[p] = np.add(grad[p], ((1/2)*params[p]*self.l2_strength))\n",
        "                #print(\"gradient after L1/L2\", grad[p])\n",
        "            for p in range(len(params)):\n",
        "                params[p] -= self.learning_rate * grad[p]\n",
        "            t += 1\n",
        "            i += 1\n",
        "            norms = np.array([np.linalg.norm(g) for g in grad])\n",
        "            if not (np.any(norms > self.epsilon)):\n",
        "                print(str(norms))\n",
        "                print(\"Norms of gradients all smaller than epsilon. Stopping.\")\n",
        "            if not (i < self.max_iters):\n",
        "                print(\"Reached max iterations.\")\n",
        "        self.iterationsPerformed = i\n",
        "        model.params = params\n",
        "        test_acc = evaluate_acc(test_y, model.predict(test_x))\n",
        "        train_acc = evaluate_acc(y, model.predict(x))\n",
        "        self.test_accuracy_per_epoch.append(test_acc)\n",
        "        self.train_accuracy_per_epoch.append(train_acc)\n",
        "        if (self.printAccuracy):\n",
        "          self.max_accuracy = max(test_acc, self.max_accuracy)\n",
        "          print(\"epoch\", epoch, \"completed. Train accuracy:\", train_acc, \". Test accuracy:\", test_acc)\n",
        "          print(\"Max accuracy is: \", self.max_accuracy)\n",
        "        return params\n",
        "\n",
        "class MLP:\n",
        "    def __init__(self, activation, activation_gradient, hidden_layers=2, hidden_units=[64, 64], dropout_p=0, weight_scale=0.01):\n",
        "        if (hidden_layers != len(hidden_units)):\n",
        "            print(\"Must have same number of hidden unit sizes as hidden layers!\")\n",
        "            exit()\n",
        "        self.hidden_layers = hidden_layers\n",
        "        self.hidden_units = hidden_units\n",
        "        self.activation = activation\n",
        "        self.activation_gradient = activation_gradient\n",
        "        self.dropout_p = dropout_p\n",
        "        self.weight_scale = weight_scale\n",
        "            \n",
        "    def init_params(self, x, y):\n",
        "        N,D = x.shape\n",
        "        _,C = y.shape\n",
        "        weight_shapes = [D]\n",
        "        weight_shapes.extend([m for m in self.hidden_units])\n",
        "        weight_shapes.append(C)\n",
        "        params_init = []\n",
        "        for i in range(len(weight_shapes)-1):\n",
        "            w = np.random.randn(weight_shapes[i]+1, weight_shapes[i+1]) * math.sqrt(2.0/weight_shapes[i]+1) * self.weight_scale\n",
        "            params_init.append(w)\n",
        "        return params_init\n",
        "\n",
        "    def fit(self, x, y, optimizer, test_x, test_y):\n",
        "        optimizer.test_accuracy_per_epoch = []\n",
        "        optimizer.train_accuracy_per_epoch = []\n",
        "        params_init = self.init_params(x, y)\n",
        "        self.params = optimizer.run(self.gradient, x, y, params_init, test_x, test_y, self)\n",
        "        return self\n",
        "\n",
        "    def gradient(self, x, y, params):\n",
        "        drop_outs = []\n",
        "        W_l = params[0]\n",
        "        N,D = x.shape\n",
        "        z_l = x\n",
        "        z_l_biased = add_bias(z_l)\n",
        "        dropped_out = np.random.random_sample(z_l_biased.shape)\n",
        "        dropped_out[dropped_out < self.dropout_p] = 0.\n",
        "        dropped_out[dropped_out >= self.dropout_p] = 1.\n",
        "        drop_outs.append(dropped_out)\n",
        "        z_l_biased *= dropped_out\n",
        "        a_l = np.dot(z_l_biased,W_l)\n",
        "        a = [a_l]\n",
        "          \n",
        "        for l in range(1, self.hidden_layers):\n",
        "            W_l = params[l]\n",
        "            z_l = self.activation(a_l)\n",
        "            z_l_biased = add_bias(z_l)\n",
        "            dropped_out = np.random.random_sample(z_l_biased.shape)\n",
        "            dropped_out[dropped_out < self.dropout_p] = 0.\n",
        "            dropped_out[dropped_out >= self.dropout_p] = 1.\n",
        "            drop_outs.append(dropped_out)\n",
        "            z_l_biased *= dropped_out\n",
        "            a_l = np.dot(z_l_biased,W_l)\n",
        "            a += [a_l]\n",
        "        \n",
        "        if self.hidden_layers > 0:\n",
        "            W_l = params[-1]\n",
        "            z_l = self.activation(a_l)\n",
        "            z_l_biased = add_bias(z_l)\n",
        "            dropped_out = np.random.random_sample(z_l_biased.shape)\n",
        "            dropped_out[dropped_out < self.dropout_p] = 0.\n",
        "            dropped_out[dropped_out >= self.dropout_p] = 1.\n",
        "            drop_outs.append(dropped_out)\n",
        "            z_l_biased *= dropped_out\n",
        "            a_l = np.dot(z_l_biased,W_l)\n",
        "        yh = softmax(a_l)\n",
        "            \n",
        "        gradient = yh-y\n",
        "        if self.hidden_layers > 0:\n",
        "            dropped_out = drop_outs.pop(-1)\n",
        "            dparams = [np.dot((dropped_out*add_bias(self.activation(a[-1]))).T, gradient)/N]\n",
        "        else:\n",
        "            dparams = []\n",
        "\n",
        "        for l in range(self.hidden_layers-1,0,-1):\n",
        "            gradient = self.activation_gradient(a[l])*np.dot(gradient, params[l+1][1:,:].T)\n",
        "            dropped_out = drop_outs.pop(-1)\n",
        "            dparams.insert(0, np.dot((dropped_out*add_bias(self.activation(a[l-1]))).T, gradient)/N)\n",
        "        \n",
        "        if self.hidden_layers > 0:\n",
        "            gradient = self.activation_gradient(a[0])*np.dot(gradient, params[1][1:,:].T)\n",
        "        dropped_out = drop_outs.pop(-1)\n",
        "        dparams.insert(0, np.dot((dropped_out*add_bias(x)).T, gradient)/N)\n",
        "\n",
        "        return dparams\n",
        "    \n",
        "    def predict(self, x):\n",
        "        yh = x\n",
        "        for i in range(len(self.params)):\n",
        "            w = self.params[i]\n",
        "            #dropout w/ weight scaling\n",
        "            w *= (1.0-self.dropout_p)\n",
        "            #don't do activation function on last weights\n",
        "            yh = add_bias(yh)\n",
        "            if i != len(self.params) - 1: yh = self.activation(np.dot(yh, w))\n",
        "            else: yh = softmax(np.dot(yh, w))\n",
        "        return yh\n",
        "\n",
        "def getData(normalize=True):\n",
        "    data_batches = []\n",
        "    directory = \"/content/cifar10/cifar-10-batches-py/\"\n",
        "\n",
        "    train_x = None\n",
        "    train_y = None\n",
        "\n",
        "    for i in range(1,6):\n",
        "        new_batch = unpickle(directory+\"data_batch_\"+str(i))\n",
        "        if train_x is None:\n",
        "            train_x = new_batch[b'data']\n",
        "            train_y = np.reshape(np.array(new_batch[b'labels']), (10000,1))\n",
        "        else:\n",
        "            train_x = np.row_stack([train_x, new_batch[b'data']])\n",
        "            train_y = np.row_stack([train_y, np.reshape(np.array(new_batch[b'labels']), (10000,1))])\n",
        "\n",
        "    test_batch = unpickle(directory+\"test_batch\")\n",
        "    test_x = test_batch[b'data']\n",
        "    test_y = test_batch[b'labels']\n",
        "\n",
        "    new_train_y = np.zeros((len(train_y), 10))\n",
        "    new_test_y = np.zeros((len(test_y), 10))\n",
        "\n",
        "    #one hot encoding labels\n",
        "    for i in range(len(train_y)):\n",
        "        new_train_y[i][train_y[i]] = 1\n",
        "    train_y = new_train_y\n",
        "\n",
        "    for i in range(len(test_y)):\n",
        "        new_test_y[i][test_y[i]] = 1\n",
        "    test_y = new_test_y\n",
        "\n",
        "    \n",
        "    #normalizing the images for each batch\n",
        "    #division by the magnitude to improve convergence speed of gradient descent \n",
        "    train_x = np.array(train_x, dtype=np.float64)\n",
        "    test_x = np.array(test_x, dtype=np.float64)\n",
        "    if normalize:\n",
        "        train_x *= 1/255\n",
        "        test_x *= 1/255\n",
        "\n",
        "    return train_x, train_y, test_x, test_y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qjzhh9sH0uT9"
      },
      "source": [
        "# Setup CNN and Custom Resnet model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xkJD7dzA0w50"
      },
      "outputs": [],
      "source": [
        "class CustomResnet18(torch.nn.Module):\n",
        "    def __init__(self, fc):\n",
        "        super(CustomResnet18, self).__init__()\n",
        "        self.resnet = models.resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
        "        self.resnet.fc = nn.Identity()\n",
        "        self.fc = fc\n",
        "        \n",
        "    def forward(self, x):\n",
        "        with torch.no_grad():\n",
        "            x = x.to(torch.device('cuda:0')).float()\n",
        "            x = self.resnet(x)\n",
        "        return self.fc(x)\n",
        "\n",
        "class CNN(torch.nn.Module):\n",
        "    def __init__(self, conv1_out_channels=20, conv1_kernel=(5,5), conv1_stride=1,\n",
        "                 conv2_out_channels=20, conv2_kernel=(5,5), conv2_stride=1):\n",
        "        super(CNN, self).__init__()\n",
        "\n",
        "        self.cnn_part = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=3, out_channels=conv1_out_channels, kernel_size=conv1_kernel, stride=conv1_stride),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=conv1_out_channels, out_channels=conv2_out_channels, kernel_size=conv2_kernel, stride=conv2_stride),\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten())\n",
        "        \n",
        "        #get output size\n",
        "        x = torch.randn(1, 3, 32, 32)\n",
        "        cnn_out_size = self.cnn_part(x).shape[1:][0]\n",
        "        self.fc_part = nn.Sequential(\n",
        "            nn.Linear(cnn_out_size, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 10),\n",
        "            nn.Softmax(dim=1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.to(torch.device('cuda:0'))\n",
        "        return self.fc_part(self.cnn_part(x.float()))\n",
        "\n",
        "\n",
        "def torch_evaluate_acc(loader, model):\n",
        "    num_correct = 0\n",
        "    num_samples = 0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:            \n",
        "            scores = model(x)\n",
        "            predictions = torch.argmax(scores, dim=1)\n",
        "            true = torch.argmax(y, dim=1)\n",
        "            diff = predictions - true\n",
        "            diff[diff != 0] = 1\n",
        "            num_false = diff.sum()\n",
        "            num_correct += (predictions.size(0)-num_false)\n",
        "            num_samples += predictions.size(0)\n",
        "        \n",
        "    return float(num_correct/num_samples)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkSLCe9xLrgT"
      },
      "source": [
        "# Hyperparameter Fine Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8eBsfzP0Ly3q"
      },
      "outputs": [],
      "source": [
        "#grid search for mlp\n",
        "train_x, train_y, test_x, test_y = getData()\n",
        "print(\"Loaded data successfully.\")\n",
        "\n",
        "#best batch size is 500\n",
        "#tested with 100 epochs\n",
        "def gridSearchBatchSize():  #estimated time to run is 15 mins\n",
        "  batch_size = [100, 500, 1500, 7500, 10000, 20000, 50000]\n",
        "  model = MLP(activation=relu, activation_gradient=relu_gradient, hidden_layers=2, hidden_units=[256, 256])\n",
        "  table = np.zeros((len(batch_size),2))\n",
        "  col_names = [\"Batch Sizes\", \"Test Accuracy\"]\n",
        "\n",
        "  for j in range(2):\n",
        "    if j!=0:\n",
        "      for i in range(len(batch_size)):\n",
        "        l = batch_size[i]\n",
        "        optimizer = GradientDescent(batch_size=l, maxEpochs=100, printAccuracy=False)\n",
        "        model.fit(train_x, train_y, optimizer, test_x, test_y)\n",
        "        table[i][j] = optimizer.max_accuracy\n",
        "    else:\n",
        "      for i in range(len(batch_size)):\n",
        "        l = batch_size[i]\n",
        "        table[i][j] = l\n",
        "\n",
        "  print(tabulate(table, headers=col_names, tablefmt=\"fancy_grid\"))\n",
        "\n",
        "#best learning rate is 0.1\n",
        "#tested with 100 epochs, 500 batch size\n",
        "def gridSearchLearningRate(): #estimated run time is 13 mins\n",
        "  learning_rates = [0.01, 0.05, 0.1, 0.5, 1]\n",
        "  model = MLP(activation=relu, activation_gradient=relu_gradient, hidden_layers=2, hidden_units=[256, 256])\n",
        "  table = np.zeros((len(learning_rates),2))\n",
        "  col_names = [\"Learning Rate\", \"Test Accuracy\"]\n",
        "\n",
        "  for i in range(len(learning_rates)):\n",
        "    l = learning_rates[i]\n",
        "    table[i][0] = l\n",
        "  for i in range(len(learning_rates)):\n",
        "      l = learning_rates[i]\n",
        "      optimizer = GradientDescent(learning_rate=l, maxEpochs=100, printAccuracy=False, batch_size=500)\n",
        "      model.fit(train_x, train_y, optimizer, test_x, test_y)\n",
        "      table[i][1] = optimizer.max_accuracy\n",
        "\n",
        "  print(tabulate(table, headers=col_names, tablefmt=\"fancy_grid\"))\n",
        "\n",
        "#best momentum is 0.5\n",
        "#tested with 100 epochs, 500 batch size, 0.1 learning rate \n",
        "def gridSearchMomentum(): #estimated run time is 14 mins\n",
        "  momentums = [0.1, 0.25, 0.5, 0.75, 0.9]\n",
        "  model = MLP(activation=relu, activation_gradient=relu_gradient, hidden_layers=2, hidden_units=[256, 256])\n",
        "  table = np.zeros((len(momentums),2))\n",
        "  col_names = [\"Momentum\", \"Test Accuracy\"]\n",
        "\n",
        "  for i in range(len(momentums)):\n",
        "    m = momentums[i]\n",
        "    table[i][0] = m\n",
        "  for i in range(len(momentums)):\n",
        "      m = momentums[i]\n",
        "      optimizer = GradientDescent(learning_rate=0.1, maxEpochs=100, printAccuracy=False, batch_size=500, momentum=m)\n",
        "      model.fit(train_x, train_y, optimizer, test_x, test_y)\n",
        "      table[i][1] = optimizer.max_accuracy\n",
        "\n",
        "  print(tabulate(table, headers=col_names, tablefmt=\"fancy_grid\"))\n",
        "\n",
        "#dropout does not improve results\n",
        "#tested with 100 epochs, 500 batch size, 0.1 learning rate and 0.5 momentum\n",
        "def gridSearchDropout():\n",
        "  dropouts = [0, 0.1, 0.25, 0.5]\n",
        "  table = np.zeros((len(dropouts),2))\n",
        "  col_names = [\"Dropout\", \"Test Accuracy\"]\n",
        "\n",
        "  for i in range(len(dropouts)):\n",
        "    m = dropouts[i]\n",
        "    table[i][0] = m\n",
        "  for i in range(len(dropouts)):\n",
        "      m = dropouts[i]\n",
        "      model = MLP(activation=relu, activation_gradient=relu_gradient, hidden_layers=2, hidden_units=[256, 256], dropout_p= m)\n",
        "      optimizer = GradientDescent(learning_rate=0.1, maxEpochs=100, printAccuracy=False, batch_size=500, momentum=0.5)\n",
        "      model.fit(train_x, train_y, optimizer, test_x, test_y)\n",
        "      table[i][1] = optimizer.max_accuracy\n",
        "\n",
        "  print(tabulate(table, headers=col_names, tablefmt=\"fancy_grid\"))\n",
        "\n",
        "def gridSearchResNet():\n",
        "    train_data = [(np.reshape(train_x[i], (3,32,32)).get(),torch.from_numpy(train_y[i].get()).to('cuda:0')) for i in range(len(train_x))]\n",
        "    test_data = [(np.reshape(test_x[i], (3,32,32)).get(),torch.from_numpy(test_y[i].get()).to('cuda:0')) for i in range(len(test_x))]\n",
        "    training_loader = torch.utils.data.DataLoader(train_data, batch_size=500, shuffle=False)\n",
        "    test_loader = torch.utils.data.DataLoader(test_data, batch_size=500, shuffle=True)\n",
        "\n",
        "    fullyConnectedLayers = [\n",
        "        nn.Sequential(\n",
        "          nn.Linear(512, 256),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(256, 10),\n",
        "          nn.Softmax(dim=1)),\n",
        "        nn.Sequential(\n",
        "          nn.Linear(512, 512),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(512, 10),\n",
        "          nn.Softmax(dim=1)),\n",
        "        nn.Sequential(\n",
        "          nn.Linear(512, 256),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(256, 256),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(256, 10),\n",
        "          nn.Softmax(dim=1)),\n",
        "        nn.Sequential(\n",
        "          nn.Linear(512, 512),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(512, 512),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(512, 10),\n",
        "          nn.Softmax(dim=1)),\n",
        "        nn.Sequential(\n",
        "          nn.Linear(512, 10),\n",
        "          nn.Softmax(dim=1))  \n",
        "    ]\n",
        "\n",
        "    table = []\n",
        "    col_names = [\"FC Layer\", \"Test Accuracy\"]\n",
        "\n",
        "    for i in range(len(fullyConnectedLayers)):\n",
        "        fc = fullyConnectedLayers[i]\n",
        "        table_entry = [str(fc), 0]\n",
        "\n",
        "        model = CustomResnet18(fc)\n",
        "        model.to(torch.device('cuda:0'))\n",
        "        loss_fn = torch.nn.CrossEntropyLoss() \n",
        "        optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.5)\n",
        "\n",
        "        def train_one_epoch(epoch_i):\n",
        "            for i, data in enumerate(training_loader):\n",
        "                inputs, labels = data\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(inputs)\n",
        "                loss = loss_fn(outputs, labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "            test_acc = torch_evaluate_acc(test_loader, model)\n",
        "            train_acc = torch_evaluate_acc(training_loader, model)\n",
        "            print(\"epoch\", epoch_i, \"completed. Train accuracy:\", train_acc, \". Test accuracy:\", test_acc)\n",
        "            return test_acc\n",
        "\n",
        "        epoch_num = 0\n",
        "        best_test_acc = 0\n",
        "        while epoch_num < 100:\n",
        "            test_acc = train_one_epoch(epoch_num)\n",
        "            best_test_acc = max(test_acc, best_test_acc)\n",
        "            epoch_num += 1\n",
        "          \n",
        "        table_entry[1] = best_test_acc\n",
        "        table.append(table_entry)\n",
        "    print(tabulate(table, headers=col_names, tablefmt=\"fancy_grid\"))\n",
        "\n",
        "def gridSearchCNN():\n",
        "    train_data = [(np.reshape(train_x[i], (3,32,32)).get(),torch.from_numpy(train_y[i].get()).to('cuda:0')) for i in range(len(train_x))]\n",
        "    test_data = [(np.reshape(test_x[i], (3,32,32)).get(),torch.from_numpy(test_y[i].get()).to('cuda:0')) for i in range(len(test_x))]\n",
        "    training_loader = torch.utils.data.DataLoader(train_data, batch_size=500, shuffle=False)\n",
        "    test_loader = torch.utils.data.DataLoader(test_data, batch_size=500, shuffle=True)\n",
        "\n",
        "    conv1_out_channels = [10, 20]\n",
        "    conv1_kernels = [5, 10]\n",
        "    conv2_out_channels = [10, 20]\n",
        "    conv2_kernels = [5, 10]\n",
        "    \n",
        "    numRows = len(conv1_out_channels)*len(conv1_kernels)*len(conv2_out_channels)*len(conv2_kernels)\n",
        "    table = np.zeros((numRows,5))\n",
        "    col_names = [\"1st Conv. Num. Channels (10 or 20)\", \"1st Conv. Kernel Size (5x5 or 10x10)\", \"2nd Conv. Num. Channels (10 or 20)\", \"2nd Conv. Kernel Size (5x5 or 10x10)\", \"Test Accuracy\"]\n",
        "    i = 0\n",
        "    for c1 in conv1_out_channels:\n",
        "        for k1 in conv1_kernels:\n",
        "                for c2 in conv2_out_channels:\n",
        "                    for k2 in conv2_kernels:\n",
        "                            table[i][0] = c1\n",
        "                            table[i][1] = k1\n",
        "                            table[i][2] = c2\n",
        "                            table[i][3] = k2\n",
        "\n",
        "                            model = CNN(conv1_out_channels=c1, conv1_kernel=k1,\n",
        "                                    conv2_out_channels=c2, conv2_kernel=k2)\n",
        "                            model.to(torch.device('cuda:0'))\n",
        "                            loss_fn = torch.nn.CrossEntropyLoss() \n",
        "                            optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.5)\n",
        "\n",
        "\n",
        "                            def train_one_epoch(epoch_i):\n",
        "                                for i, data in enumerate(training_loader):\n",
        "                                    inputs, labels = data\n",
        "                                    optimizer.zero_grad()\n",
        "                                    outputs = model(inputs)\n",
        "                                    loss = loss_fn(outputs, labels)\n",
        "                                    loss.backward()\n",
        "                                    optimizer.step()\n",
        "                                test_acc = torch_evaluate_acc(test_loader, model)\n",
        "                                train_acc = torch_evaluate_acc(training_loader, model)\n",
        "                                print(\"epoch\", epoch_i, \"completed. Train accuracy:\", train_acc, \". Test accuracy:\", test_acc)\n",
        "                                return test_acc\n",
        "\n",
        "                            epoch_num = 0\n",
        "                            best_test_acc = 0\n",
        "                            while epoch_num < 50:\n",
        "                                test_acc = train_one_epoch(epoch_num)\n",
        "                                best_test_acc = max(test_acc, best_test_acc)\n",
        "                                epoch_num += 1\n",
        "                              \n",
        "                            table[i][4] = best_test_acc\n",
        "                            i += 1\n",
        "                            print(tabulate(table, headers=col_names, tablefmt=\"fancy_grid\"))\n",
        "\n",
        "###Uncomment one of these to test\n",
        "#gridSearchBatchSize()\n",
        "#gridSearchLearningRate()\n",
        "#gridSearchMomentum()\n",
        "#gridSearchDropout()\n",
        "#gridSearchResNet()\n",
        "gridSearchCNN()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjF24ftuAtrP"
      },
      "source": [
        "# Experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pf0bQVo76Lr2"
      },
      "source": [
        "EXPERIMENT 1 - Varying the number of hidden layers (0,1,2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Iy5dsMDa6NST"
      },
      "outputs": [],
      "source": [
        "train_x, train_y, test_x, test_y = getData()\n",
        "print(\"Loaded data successfully.\")\n",
        "\n",
        "optimizer = GradientDescent(learning_rate=.1, batch_size=500, momentum=0.5)\n",
        "model = MLP(activation=None, activation_gradient=None, hidden_layers=0, hidden_units=[])\n",
        "model.fit(train_x, train_y, optimizer, test_x, test_y)\n",
        "\n",
        "plt.figure(0)\n",
        "plt.plot(optimizer.train_accuracy_per_epoch, color=\"blue\")\n",
        "plt.plot(optimizer.test_accuracy_per_epoch, color=\"red\")\n",
        "plt.title(\"No hidden layers\")\n",
        "plt.xlabel('Num. Epochs')\n",
        "plt.ylabel('Accuracy (blue=train, red=test)')\n",
        "plt.show()\n",
        "\n",
        "model = MLP(activation=relu, activation_gradient=relu_gradient, hidden_layers=1, hidden_units=[256])\n",
        "model.fit(train_x, train_y, optimizer, test_x, test_y)\n",
        "\n",
        "plt.figure(1)\n",
        "plt.plot(optimizer.train_accuracy_per_epoch, color=\"blue\")\n",
        "plt.plot(optimizer.test_accuracy_per_epoch, color=\"red\")\n",
        "plt.title(\"1 hidden layer with 256 units, ReLu activation\")\n",
        "plt.xlabel('Num. Epochs')\n",
        "plt.ylabel('Accuracy (blue=train, red=test)')\n",
        "plt.show()\n",
        "\n",
        "model = MLP(activation=relu, activation_gradient=relu_gradient, hidden_layers=2, hidden_units=[256, 256])\n",
        "model.fit(train_x, train_y, optimizer, test_x, test_y)\n",
        "\n",
        "plt.figure(2)\n",
        "plt.plot(optimizer.train_accuracy_per_epoch, color=\"blue\")\n",
        "plt.plot(optimizer.test_accuracy_per_epoch, color=\"red\")\n",
        "plt.title(\"2 hidden layers with 256 units each, ReLu activation\")\n",
        "plt.xlabel('Num. Epochs')\n",
        "plt.ylabel('Accuracy (blue=train, red=test)')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgO6epbK6-g3"
      },
      "source": [
        "EXPERIMENT 2 - Activation function TanH and LeakyReLu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EClObvac6N6c"
      },
      "outputs": [],
      "source": [
        "train_x, train_y, test_x, test_y = getData()\n",
        "print(\"Loaded data successfully.\")\n",
        "\n",
        "optimizer = GradientDescent(learning_rate=.1, batch_size=500, momentum=0.5)\n",
        "model = MLP(activation=leaky_relu, activation_gradient=leaky_relu_gradient, hidden_layers=2, hidden_units=[256, 256])\n",
        "model.fit(train_x, train_y, optimizer, test_x, test_y)\n",
        "\n",
        "plt.figure(0)\n",
        "plt.plot(optimizer.train_accuracy_per_epoch, color=\"blue\")\n",
        "plt.plot(optimizer.test_accuracy_per_epoch, color=\"red\")\n",
        "plt.title(\"Leaky ReLu Activation\")\n",
        "plt.xlabel('Num. Epochs')\n",
        "plt.ylabel('Accuracy (blue=train, red=test)')\n",
        "plt.show()\n",
        "\n",
        "model = MLP(activation=hyperbolic_tan, activation_gradient=hyperbolic_tan_gradient, hidden_layers=2, hidden_units=[256, 256])\n",
        "model.fit(train_x, train_y, optimizer, test_x, test_y)\n",
        "\n",
        "plt.figure(1)\n",
        "plt.plot(optimizer.train_accuracy_per_epoch, color=\"blue\")\n",
        "plt.plot(optimizer.test_accuracy_per_epoch, color=\"red\")\n",
        "plt.title(\"Tanh Activation\")\n",
        "plt.xlabel('Num. Epochs')\n",
        "plt.ylabel('Accuracy (blue=train, red=test)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3C5CllBb9KAy"
      },
      "source": [
        "EXPERIMENT 3 - Adding L1 and L2 regularizations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l02wrgfV9OOv"
      },
      "outputs": [],
      "source": [
        "train_x, train_y, test_x, test_y = getData()\n",
        "print(\"Loaded data successfully.\")\n",
        "model = MLP(activation=relu, activation_gradient=relu_gradient, hidden_layers=2, hidden_units=[256, 256], weight_scale=0.01)\n",
        "\n",
        "figure_num = 0\n",
        "l1_strengths = [0.0001, 0.00005, 0.00025]\n",
        "l2_strengths = [0.0001, 0.00005, 0.00025]\n",
        "\n",
        "for i in range(len(l1_strengths)):\n",
        "    l1 = l1_strengths[i]\n",
        "    optimizer = GradientDescent(learning_rate=.1, batch_size=500, momentum=0.5, l1_strength=l1, maxEpochs=300)\n",
        "    model.fit(train_x, train_y, optimizer, test_x, test_y)\n",
        "\n",
        "    plt.figure(figure_num)\n",
        "    figure_num += 1\n",
        "    plt.plot(optimizer.train_accuracy_per_epoch, color=\"blue\")\n",
        "    plt.plot(optimizer.test_accuracy_per_epoch, color=\"red\")\n",
        "    plt.title(\"L1 Regularization @ \" + str(l1))\n",
        "    plt.xlabel('Num. Epochs')\n",
        "    plt.ylabel('Accuracy (blue=train, red=test)')\n",
        "    plt.show()\n",
        "\n",
        "for i in range(len(l2_strengths)):\n",
        "    l2 = l2_strengths[i]\n",
        "    optimizer = GradientDescent(learning_rate=.1, batch_size=500, momentum=0.5, l2_strength=l2, maxEpochs=300)\n",
        "    model.fit(train_x, train_y, optimizer, test_x, test_y)\n",
        "\n",
        "    plt.figure(figure_num)\n",
        "    figure_num += 1\n",
        "    plt.plot(optimizer.train_accuracy_per_epoch, color=\"blue\")\n",
        "    plt.plot(optimizer.test_accuracy_per_epoch, color=\"red\")\n",
        "    plt.title(\"L2 Regularization @ \" + str(l2))\n",
        "    plt.xlabel('Num. Epochs')\n",
        "    plt.ylabel('Accuracy (blue=train, red=test)')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HB4hBKnQASTD"
      },
      "source": [
        "EXPERIMENT 4 - Unormalized Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DZ3eQsIhATVv"
      },
      "outputs": [],
      "source": [
        "train_x, train_y, test_x, test_y = getData(normalize=False)\n",
        "print(\"Loaded data successfully.\")\n",
        "optimizer = GradientDescent(learning_rate=.1/255, batch_size=500, momentum=0.5, epsilon=1e-10)\n",
        "\n",
        "model = MLP(activation=relu, activation_gradient=relu_gradient, hidden_layers=2, hidden_units=[256, 256], weight_scale=0.01/255)\n",
        "model.fit(train_x, train_y, optimizer, test_x, test_y)\n",
        "\n",
        "plt.figure(0)\n",
        "plt.plot(optimizer.train_accuracy_per_epoch, color=\"blue\")\n",
        "plt.plot(optimizer.test_accuracy_per_epoch, color=\"red\")\n",
        "plt.title(\"2 hidden layers with 256 units each, ReLu activation (unnormalized images)\")\n",
        "plt.xlabel('Num. Epochs')\n",
        "plt.ylabel('Accuracy (blue=train, red=test)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ox5eB-FMBgEG"
      },
      "source": [
        "EXPERIMENT 5 - CNN training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XD_R2m-NBhGz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b9bbb33e-3f03-4b8b-928b-f1a942c9e079"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded data successfully.\n",
            "epoch 0 completed. Train accuracy: 0.1196799948811531 . Test accuracy: 0.11789999902248383\n",
            "epoch 1 completed. Train accuracy: 0.2440599948167801 . Test accuracy: 0.2500999867916107\n",
            "epoch 2 completed. Train accuracy: 0.2947999835014343 . Test accuracy: 0.296999990940094\n",
            "epoch 3 completed. Train accuracy: 0.32510000467300415 . Test accuracy: 0.32690000534057617\n",
            "epoch 4 completed. Train accuracy: 0.3527999818325043 . Test accuracy: 0.3547999858856201\n",
            "epoch 5 completed. Train accuracy: 0.3724599778652191 . Test accuracy: 0.37379997968673706\n",
            "epoch 6 completed. Train accuracy: 0.38773998618125916 . Test accuracy: 0.3878999948501587\n",
            "epoch 7 completed. Train accuracy: 0.40411999821662903 . Test accuracy: 0.3999999761581421\n",
            "epoch 8 completed. Train accuracy: 0.41543999314308167 . Test accuracy: 0.41169998049736023\n",
            "epoch 9 completed. Train accuracy: 0.4205399751663208 . Test accuracy: 0.4235999882221222\n",
            "epoch 10 completed. Train accuracy: 0.4410199820995331 . Test accuracy: 0.4341999888420105\n",
            "epoch 11 completed. Train accuracy: 0.4632200002670288 . Test accuracy: 0.45579999685287476\n",
            "epoch 12 completed. Train accuracy: 0.47999998927116394 . Test accuracy: 0.4693000018596649\n",
            "epoch 13 completed. Train accuracy: 0.48385998606681824 . Test accuracy: 0.47669997811317444\n",
            "epoch 14 completed. Train accuracy: 0.5050399899482727 . Test accuracy: 0.4943999946117401\n",
            "epoch 15 completed. Train accuracy: 0.5102999806404114 . Test accuracy: 0.4950999915599823\n",
            "epoch 16 completed. Train accuracy: 0.5240799784660339 . Test accuracy: 0.5002999901771545\n",
            "epoch 17 completed. Train accuracy: 0.5221799612045288 . Test accuracy: 0.4974999725818634\n",
            "epoch 18 completed. Train accuracy: 0.5414199829101562 . Test accuracy: 0.5110999941825867\n",
            "epoch 19 completed. Train accuracy: 0.553820013999939 . Test accuracy: 0.5205000042915344\n",
            "epoch 20 completed. Train accuracy: 0.5504599809646606 . Test accuracy: 0.5199999809265137\n",
            "epoch 21 completed. Train accuracy: 0.5601400136947632 . Test accuracy: 0.522599995136261\n",
            "epoch 22 completed. Train accuracy: 0.566540002822876 . Test accuracy: 0.520799994468689\n",
            "epoch 23 completed. Train accuracy: 0.572439968585968 . Test accuracy: 0.5320999622344971\n",
            "epoch 24 completed. Train accuracy: 0.5762799978256226 . Test accuracy: 0.5351999998092651\n",
            "epoch 25 completed. Train accuracy: 0.585159957408905 . Test accuracy: 0.5321999788284302\n",
            "epoch 26 completed. Train accuracy: 0.6036399602890015 . Test accuracy: 0.5458999872207642\n",
            "epoch 27 completed. Train accuracy: 0.597279965877533 . Test accuracy: 0.5428000092506409\n",
            "epoch 28 completed. Train accuracy: 0.6210199594497681 . Test accuracy: 0.5471999645233154\n",
            "epoch 29 completed. Train accuracy: 0.6116799712181091 . Test accuracy: 0.5415999889373779\n",
            "epoch 30 completed. Train accuracy: 0.6301800012588501 . Test accuracy: 0.5504999756813049\n",
            "epoch 31 completed. Train accuracy: 0.6171799898147583 . Test accuracy: 0.538599967956543\n",
            "epoch 32 completed. Train accuracy: 0.634939968585968 . Test accuracy: 0.5504999756813049\n",
            "epoch 33 completed. Train accuracy: 0.6520199775695801 . Test accuracy: 0.5623999834060669\n",
            "epoch 34 completed. Train accuracy: 0.6536399722099304 . Test accuracy: 0.5661999583244324\n",
            "epoch 35 completed. Train accuracy: 0.6490799784660339 . Test accuracy: 0.5591999888420105\n",
            "epoch 36 completed. Train accuracy: 0.6458799839019775 . Test accuracy: 0.5552999973297119\n",
            "epoch 37 completed. Train accuracy: 0.666159987449646 . Test accuracy: 0.5645999908447266\n",
            "epoch 38 completed. Train accuracy: 0.6656599640846252 . Test accuracy: 0.5598999857902527\n",
            "epoch 39 completed. Train accuracy: 0.6690599918365479 . Test accuracy: 0.5651999711990356\n",
            "epoch 40 completed. Train accuracy: 0.6690199971199036 . Test accuracy: 0.5644999742507935\n",
            "epoch 41 completed. Train accuracy: 0.6824199557304382 . Test accuracy: 0.5684999823570251\n",
            "epoch 42 completed. Train accuracy: 0.6886199712753296 . Test accuracy: 0.5706999897956848\n",
            "epoch 43 completed. Train accuracy: 0.7007399797439575 . Test accuracy: 0.5753999948501587\n",
            "epoch 44 completed. Train accuracy: 0.6938799619674683 . Test accuracy: 0.5715000033378601\n",
            "epoch 45 completed. Train accuracy: 0.6928600072860718 . Test accuracy: 0.5672999620437622\n",
            "epoch 46 completed. Train accuracy: 0.7005199790000916 . Test accuracy: 0.5684999823570251\n",
            "epoch 47 completed. Train accuracy: 0.7127000093460083 . Test accuracy: 0.5773000121116638\n",
            "epoch 48 completed. Train accuracy: 0.7076399922370911 . Test accuracy: 0.574400007724762\n",
            "epoch 49 completed. Train accuracy: 0.7266799807548523 . Test accuracy: 0.5866000056266785\n",
            "epoch 50 completed. Train accuracy: 0.7257399559020996 . Test accuracy: 0.5746999979019165\n",
            "epoch 51 completed. Train accuracy: 0.7281399965286255 . Test accuracy: 0.5773000121116638\n",
            "epoch 52 completed. Train accuracy: 0.7350800037384033 . Test accuracy: 0.5828999876976013\n",
            "epoch 53 completed. Train accuracy: 0.7224599719047546 . Test accuracy: 0.576200008392334\n",
            "epoch 54 completed. Train accuracy: 0.7402999997138977 . Test accuracy: 0.579800009727478\n",
            "epoch 55 completed. Train accuracy: 0.7468799948692322 . Test accuracy: 0.5801999568939209\n",
            "epoch 56 completed. Train accuracy: 0.7497400045394897 . Test accuracy: 0.5781999826431274\n",
            "epoch 57 completed. Train accuracy: 0.7195799946784973 . Test accuracy: 0.5618999600410461\n",
            "epoch 58 completed. Train accuracy: 0.7556399703025818 . Test accuracy: 0.5809000134468079\n",
            "epoch 59 completed. Train accuracy: 0.7359399795532227 . Test accuracy: 0.5697000026702881\n",
            "epoch 60 completed. Train accuracy: 0.7602799534797668 . Test accuracy: 0.5827000141143799\n",
            "epoch 61 completed. Train accuracy: 0.7638999819755554 . Test accuracy: 0.5781999826431274\n",
            "epoch 62 completed. Train accuracy: 0.7807799577713013 . Test accuracy: 0.5895999670028687\n",
            "epoch 63 completed. Train accuracy: 0.7823999524116516 . Test accuracy: 0.5871999859809875\n",
            "epoch 64 completed. Train accuracy: 0.7826799750328064 . Test accuracy: 0.5852000117301941\n",
            "epoch 65 completed. Train accuracy: 0.7866599559783936 . Test accuracy: 0.5863999724388123\n",
            "epoch 66 completed. Train accuracy: 0.7732399702072144 . Test accuracy: 0.5799999833106995\n",
            "epoch 67 completed. Train accuracy: 0.7862799763679504 . Test accuracy: 0.589199960231781\n",
            "epoch 68 completed. Train accuracy: 0.7875199913978577 . Test accuracy: 0.5791999697685242\n",
            "epoch 69 completed. Train accuracy: 0.7895399928092957 . Test accuracy: 0.5852000117301941\n",
            "epoch 70 completed. Train accuracy: 0.7859999537467957 . Test accuracy: 0.5837000012397766\n",
            "epoch 71 completed. Train accuracy: 0.7953199744224548 . Test accuracy: 0.589199960231781\n",
            "epoch 72 completed. Train accuracy: 0.7763800024986267 . Test accuracy: 0.5778999924659729\n",
            "epoch 73 completed. Train accuracy: 0.7946999669075012 . Test accuracy: 0.5846999883651733\n",
            "epoch 74 completed. Train accuracy: 0.789900004863739 . Test accuracy: 0.5746999979019165\n",
            "epoch 75 completed. Train accuracy: 0.795699954032898 . Test accuracy: 0.5841999650001526\n",
            "epoch 76 completed. Train accuracy: 0.8010799884796143 . Test accuracy: 0.5849999785423279\n",
            "epoch 77 completed. Train accuracy: 0.7898199558258057 . Test accuracy: 0.5774999856948853\n",
            "epoch 78 completed. Train accuracy: 0.7918999791145325 . Test accuracy: 0.5769000053405762\n",
            "epoch 79 completed. Train accuracy: 0.810759961605072 . Test accuracy: 0.5866999626159668\n",
            "epoch 80 completed. Train accuracy: 0.8077399730682373 . Test accuracy: 0.5778999924659729\n",
            "epoch 81 completed. Train accuracy: 0.815559983253479 . Test accuracy: 0.5866000056266785\n",
            "epoch 82 completed. Train accuracy: 0.8190000057220459 . Test accuracy: 0.5859999656677246\n",
            "epoch 83 completed. Train accuracy: 0.8215599656105042 . Test accuracy: 0.5902999639511108\n",
            "epoch 84 completed. Train accuracy: 0.8146799802780151 . Test accuracy: 0.5877000093460083\n",
            "epoch 85 completed. Train accuracy: 0.8262199759483337 . Test accuracy: 0.5888000130653381\n",
            "epoch 86 completed. Train accuracy: 0.8213399648666382 . Test accuracy: 0.585099995136261\n",
            "epoch 87 completed. Train accuracy: 0.8213599920272827 . Test accuracy: 0.5848999619483948\n",
            "epoch 88 completed. Train accuracy: 0.8227799534797668 . Test accuracy: 0.5888000130653381\n",
            "epoch 89 completed. Train accuracy: 0.8214199542999268 . Test accuracy: 0.5842999815940857\n",
            "epoch 90 completed. Train accuracy: 0.836359977722168 . Test accuracy: 0.5941999554634094\n",
            "epoch 91 completed. Train accuracy: 0.8307799696922302 . Test accuracy: 0.5902000069618225\n",
            "epoch 92 completed. Train accuracy: 0.8327800035476685 . Test accuracy: 0.592199981212616\n",
            "epoch 93 completed. Train accuracy: 0.838699996471405 . Test accuracy: 0.5936999917030334\n",
            "epoch 94 completed. Train accuracy: 0.8347199559211731 . Test accuracy: 0.5874999761581421\n",
            "epoch 95 completed. Train accuracy: 0.8428199887275696 . Test accuracy: 0.5949999690055847\n",
            "epoch 96 completed. Train accuracy: 0.8424999713897705 . Test accuracy: 0.5899999737739563\n",
            "epoch 97 completed. Train accuracy: 0.8443199992179871 . Test accuracy: 0.5945999622344971\n",
            "epoch 98 completed. Train accuracy: 0.8480199575424194 . Test accuracy: 0.5950999855995178\n",
            "epoch 99 completed. Train accuracy: 0.8492599725723267 . Test accuracy: 0.5934000015258789\n",
            "epoch 100 completed. Train accuracy: 0.8476199507713318 . Test accuracy: 0.5920000076293945\n",
            "epoch 101 completed. Train accuracy: 0.8514999747276306 . Test accuracy: 0.5985999703407288\n",
            "epoch 102 completed. Train accuracy: 0.8468999862670898 . Test accuracy: 0.5928999781608582\n",
            "epoch 103 completed. Train accuracy: 0.8463599681854248 . Test accuracy: 0.5929999947547913\n",
            "epoch 104 completed. Train accuracy: 0.8517999649047852 . Test accuracy: 0.5918999910354614\n",
            "epoch 105 completed. Train accuracy: 0.8537600040435791 . Test accuracy: 0.5974000096321106\n",
            "epoch 106 completed. Train accuracy: 0.8433199524879456 . Test accuracy: 0.5864999890327454\n",
            "epoch 107 completed. Train accuracy: 0.854699969291687 . Test accuracy: 0.5940999984741211\n",
            "epoch 108 completed. Train accuracy: 0.8567599654197693 . Test accuracy: 0.5952999591827393\n",
            "epoch 109 completed. Train accuracy: 0.8565999865531921 . Test accuracy: 0.5925999879837036\n",
            "epoch 110 completed. Train accuracy: 0.8573199510574341 . Test accuracy: 0.5907999873161316\n",
            "epoch 111 completed. Train accuracy: 0.8575399518013 . Test accuracy: 0.5935999751091003\n",
            "epoch 112 completed. Train accuracy: 0.8592999577522278 . Test accuracy: 0.5938000082969666\n",
            "epoch 113 completed. Train accuracy: 0.8579199910163879 . Test accuracy: 0.5900999903678894\n",
            "epoch 114 completed. Train accuracy: 0.8565399646759033 . Test accuracy: 0.5945999622344971\n",
            "epoch 115 completed. Train accuracy: 0.8592599630355835 . Test accuracy: 0.5992000102996826\n",
            "epoch 116 completed. Train accuracy: 0.859499990940094 . Test accuracy: 0.592799961566925\n",
            "epoch 117 completed. Train accuracy: 0.8600399494171143 . Test accuracy: 0.5949000120162964\n",
            "epoch 118 completed. Train accuracy: 0.8627600073814392 . Test accuracy: 0.5934000015258789\n",
            "epoch 119 completed. Train accuracy: 0.8629999756813049 . Test accuracy: 0.5945999622344971\n",
            "epoch 120 completed. Train accuracy: 0.8634199500083923 . Test accuracy: 0.5942999720573425\n",
            "epoch 121 completed. Train accuracy: 0.8650199770927429 . Test accuracy: 0.592799961566925\n",
            "epoch 122 completed. Train accuracy: 0.8643400073051453 . Test accuracy: 0.592799961566925\n",
            "epoch 123 completed. Train accuracy: 0.8650599718093872 . Test accuracy: 0.5935999751091003\n",
            "epoch 124 completed. Train accuracy: 0.8651999831199646 . Test accuracy: 0.5956000089645386\n",
            "epoch 125 completed. Train accuracy: 0.8666799664497375 . Test accuracy: 0.5956000089645386\n",
            "epoch 126 completed. Train accuracy: 0.8672199845314026 . Test accuracy: 0.5935999751091003\n",
            "epoch 127 completed. Train accuracy: 0.8664199709892273 . Test accuracy: 0.5949000120162964\n",
            "epoch 128 completed. Train accuracy: 0.8669399619102478 . Test accuracy: 0.592799961566925\n",
            "epoch 129 completed. Train accuracy: 0.8673799633979797 . Test accuracy: 0.5941999554634094\n",
            "epoch 130 completed. Train accuracy: 0.8676599860191345 . Test accuracy: 0.5945000052452087\n",
            "epoch 131 completed. Train accuracy: 0.8679599761962891 . Test accuracy: 0.5967999696731567\n",
            "epoch 132 completed. Train accuracy: 0.868179976940155 . Test accuracy: 0.5954999923706055\n",
            "epoch 133 completed. Train accuracy: 0.8682000041007996 . Test accuracy: 0.5956000089645386\n",
            "epoch 134 completed. Train accuracy: 0.8689799904823303 . Test accuracy: 0.5935999751091003\n",
            "epoch 135 completed. Train accuracy: 0.869379997253418 . Test accuracy: 0.597000002861023\n",
            "epoch 136 completed. Train accuracy: 0.869379997253418 . Test accuracy: 0.593999981880188\n",
            "epoch 137 completed. Train accuracy: 0.869439959526062 . Test accuracy: 0.5956999659538269\n",
            "epoch 138 completed. Train accuracy: 0.869879961013794 . Test accuracy: 0.5942999720573425\n",
            "epoch 139 completed. Train accuracy: 0.8698399662971497 . Test accuracy: 0.5945000052452087\n",
            "epoch 140 completed. Train accuracy: 0.8700999617576599 . Test accuracy: 0.5956000089645386\n",
            "epoch 141 completed. Train accuracy: 0.8701399564743042 . Test accuracy: 0.5943999886512756\n",
            "epoch 142 completed. Train accuracy: 0.870199978351593 . Test accuracy: 0.5956999659538269\n",
            "epoch 143 completed. Train accuracy: 0.8702999949455261 . Test accuracy: 0.5971999764442444\n",
            "epoch 144 completed. Train accuracy: 0.8703999519348145 . Test accuracy: 0.5971999764442444\n",
            "epoch 145 completed. Train accuracy: 0.8704400062561035 . Test accuracy: 0.59579998254776\n",
            "epoch 146 completed. Train accuracy: 0.8705199956893921 . Test accuracy: 0.5971999764442444\n",
            "epoch 147 completed. Train accuracy: 0.8705999851226807 . Test accuracy: 0.5949000120162964\n",
            "epoch 148 completed. Train accuracy: 0.8706199526786804 . Test accuracy: 0.5967999696731567\n",
            "epoch 149 completed. Train accuracy: 0.8707999587059021 . Test accuracy: 0.5956999659538269\n",
            "epoch 150 completed. Train accuracy: 0.8707599639892578 . Test accuracy: 0.5949000120162964\n",
            "epoch 151 completed. Train accuracy: 0.8710199594497681 . Test accuracy: 0.5971999764442444\n",
            "epoch 152 completed. Train accuracy: 0.8711199760437012 . Test accuracy: 0.5967999696731567\n",
            "epoch 153 completed. Train accuracy: 0.8711599707603455 . Test accuracy: 0.5981000065803528\n",
            "epoch 154 completed. Train accuracy: 0.8711999654769897 . Test accuracy: 0.5967999696731567\n",
            "epoch 155 completed. Train accuracy: 0.8713399767875671 . Test accuracy: 0.5983999967575073\n",
            "epoch 156 completed. Train accuracy: 0.8713600039482117 . Test accuracy: 0.5974000096321106\n",
            "epoch 157 completed. Train accuracy: 0.8714399933815002 . Test accuracy: 0.5967000126838684\n",
            "epoch 158 completed. Train accuracy: 0.8715199828147888 . Test accuracy: 0.597599983215332\n",
            "epoch 159 completed. Train accuracy: 0.8716399669647217 . Test accuracy: 0.5982999801635742\n",
            "epoch 160 completed. Train accuracy: 0.8716999888420105 . Test accuracy: 0.5989999771118164\n",
            "epoch 161 completed. Train accuracy: 0.8717799782752991 . Test accuracy: 0.5979999899864197\n",
            "epoch 162 completed. Train accuracy: 0.8718199729919434 . Test accuracy: 0.5972999930381775\n",
            "epoch 163 completed. Train accuracy: 0.8718599677085876 . Test accuracy: 0.5974999666213989\n",
            "epoch 164 completed. Train accuracy: 0.8719399571418762 . Test accuracy: 0.5960999727249146\n",
            "epoch 165 completed. Train accuracy: 0.8719799518585205 . Test accuracy: 0.5970999598503113\n",
            "epoch 166 completed. Train accuracy: 0.8719599843025208 . Test accuracy: 0.5974000096321106\n",
            "epoch 167 completed. Train accuracy: 0.8721199631690979 . Test accuracy: 0.5970999598503113\n",
            "epoch 168 completed. Train accuracy: 0.8721599578857422 . Test accuracy: 0.5970999598503113\n",
            "epoch 169 completed. Train accuracy: 0.8721999526023865 . Test accuracy: 0.597599983215332\n",
            "epoch 170 completed. Train accuracy: 0.872219979763031 . Test accuracy: 0.5978999733924866\n",
            "epoch 171 completed. Train accuracy: 0.872219979763031 . Test accuracy: 0.598800003528595\n",
            "epoch 172 completed. Train accuracy: 0.872219979763031 . Test accuracy: 0.5979999899864197\n",
            "epoch 173 completed. Train accuracy: 0.8722400069236755 . Test accuracy: 0.5974000096321106\n",
            "epoch 174 completed. Train accuracy: 0.8722599744796753 . Test accuracy: 0.5979999899864197\n",
            "epoch 175 completed. Train accuracy: 0.8722999691963196 . Test accuracy: 0.5976999998092651\n",
            "epoch 176 completed. Train accuracy: 0.8723599910736084 . Test accuracy: 0.5972999930381775\n",
            "epoch 177 completed. Train accuracy: 0.8723599910736084 . Test accuracy: 0.597599983215332\n",
            "epoch 178 completed. Train accuracy: 0.8723799586296082 . Test accuracy: 0.5970999598503113\n",
            "epoch 179 completed. Train accuracy: 0.872439980506897 . Test accuracy: 0.5956999659538269\n",
            "epoch 180 completed. Train accuracy: 0.8725399971008301 . Test accuracy: 0.5979999899864197\n",
            "epoch 181 completed. Train accuracy: 0.8726199865341187 . Test accuracy: 0.5985999703407288\n",
            "epoch 182 completed. Train accuracy: 0.8727200031280518 . Test accuracy: 0.5961999893188477\n",
            "epoch 183 completed. Train accuracy: 0.8727200031280518 . Test accuracy: 0.5959999561309814\n",
            "epoch 184 completed. Train accuracy: 0.872759997844696 . Test accuracy: 0.5964999794960022\n",
            "epoch 185 completed. Train accuracy: 0.8728399872779846 . Test accuracy: 0.5938000082969666\n",
            "epoch 186 completed. Train accuracy: 0.8728599548339844 . Test accuracy: 0.5963999629020691\n",
            "epoch 187 completed. Train accuracy: 0.8728999495506287 . Test accuracy: 0.5956999659538269\n",
            "epoch 188 completed. Train accuracy: 0.872979998588562 . Test accuracy: 0.5945000052452087\n",
            "epoch 189 completed. Train accuracy: 0.873039960861206 . Test accuracy: 0.5960999727249146\n",
            "epoch 190 completed. Train accuracy: 0.8730599880218506 . Test accuracy: 0.5963000059127808\n",
            "epoch 191 completed. Train accuracy: 0.8730999827384949 . Test accuracy: 0.5938000082969666\n",
            "epoch 192 completed. Train accuracy: 0.8730999827384949 . Test accuracy: 0.5949999690055847\n",
            "epoch 193 completed. Train accuracy: 0.8731799721717834 . Test accuracy: 0.5963999629020691\n",
            "epoch 194 completed. Train accuracy: 0.8732199668884277 . Test accuracy: 0.5956000089645386\n",
            "epoch 195 completed. Train accuracy: 0.8732399940490723 . Test accuracy: 0.5960999727249146\n",
            "epoch 196 completed. Train accuracy: 0.8732799887657166 . Test accuracy: 0.59579998254776\n",
            "epoch 197 completed. Train accuracy: 0.8733199834823608 . Test accuracy: 0.5956999659538269\n",
            "epoch 198 completed. Train accuracy: 0.8733399510383606 . Test accuracy: 0.5956999659538269\n",
            "epoch 199 completed. Train accuracy: 0.873420000076294 . Test accuracy: 0.5952000021934509\n",
            "epoch 200 completed. Train accuracy: 0.8734399676322937 . Test accuracy: 0.5960999727249146\n",
            "epoch 201 completed. Train accuracy: 0.8734599947929382 . Test accuracy: 0.5963999629020691\n",
            "epoch 202 completed. Train accuracy: 0.8734999895095825 . Test accuracy: 0.5952000021934509\n",
            "epoch 203 completed. Train accuracy: 0.8735199570655823 . Test accuracy: 0.5938000082969666\n",
            "epoch 204 completed. Train accuracy: 0.8735199570655823 . Test accuracy: 0.5936999917030334\n",
            "epoch 205 completed. Train accuracy: 0.8735599517822266 . Test accuracy: 0.5963000059127808\n",
            "epoch 206 completed. Train accuracy: 0.8736199736595154 . Test accuracy: 0.5959999561309814\n",
            "epoch 207 completed. Train accuracy: 0.8736400008201599 . Test accuracy: 0.5945999622344971\n",
            "epoch 208 completed. Train accuracy: 0.8736799955368042 . Test accuracy: 0.5947999954223633\n",
            "epoch 209 completed. Train accuracy: 0.873699963092804 . Test accuracy: 0.5950999855995178\n",
            "epoch 210 completed. Train accuracy: 0.873699963092804 . Test accuracy: 0.5958999991416931\n",
            "epoch 211 completed. Train accuracy: 0.873699963092804 . Test accuracy: 0.5963999629020691\n",
            "epoch 212 completed. Train accuracy: 0.8737199902534485 . Test accuracy: 0.5947999954223633\n",
            "epoch 213 completed. Train accuracy: 0.8737599849700928 . Test accuracy: 0.5956000089645386\n",
            "epoch 214 completed. Train accuracy: 0.8737799525260925 . Test accuracy: 0.5965999960899353\n",
            "Max test accuracy is 0.5992000102996826\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "train_x, train_y, test_x, test_y = getData()\n",
        "print(\"Loaded data successfully.\")\n",
        "train_data = [(np.reshape(train_x[i], (3,32,32)).get(),torch.from_numpy(train_y[i].get()).to('cuda:0')) for i in range(len(train_x))]\n",
        "test_data = [(np.reshape(test_x[i], (3,32,32)).get(),torch.from_numpy(test_y[i].get()).to('cuda:0')) for i in range(len(test_x))]\n",
        "\n",
        "training_loader = torch.utils.data.DataLoader(train_data, batch_size=500, shuffle=False)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=500, shuffle=True)\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "model = CNN(conv1_out_channels=20, conv1_kernel=(5,5),\n",
        "                 conv2_out_channels=20, conv2_kernel=(10,10))\n",
        "model.to(torch.device('cuda:0'))\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.5)\n",
        "\n",
        "def train_one_epoch(epoch_i):\n",
        "    for i, data in enumerate(training_loader):\n",
        "        inputs, labels = data\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = loss_fn(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    test_acc = torch_evaluate_acc(test_loader, model)\n",
        "    train_acc = torch_evaluate_acc(training_loader, model)\n",
        "    print(\"epoch\", epoch_i, \"completed. Train accuracy:\", train_acc, \". Test accuracy:\", test_acc)\n",
        "    return test_acc, train_acc\n",
        "\n",
        "train_accuracy_per_epoch = []\n",
        "test_accuracy_per_epoch = []\n",
        "epoch_num = 0\n",
        "last_test_acc_increase = 0\n",
        "best_test_acc = 0\n",
        "while epoch_num - last_test_acc_increase < 100:\n",
        "    test_acc, train_acc = train_one_epoch(epoch_num)\n",
        "    test_accuracy_per_epoch.append(test_acc)\n",
        "    train_accuracy_per_epoch.append(train_acc)\n",
        "    if test_acc > best_test_acc: last_test_acc_increase = epoch_num\n",
        "    best_test_acc = max(test_acc, best_test_acc)\n",
        "    epoch_num += 1\n",
        "\n",
        "print(\"Max test accuracy is\", best_test_acc)\n",
        "\n",
        "plt.figure(0)\n",
        "plt.plot(train_accuracy_per_epoch, color=\"blue\")\n",
        "plt.plot(test_accuracy_per_epoch, color=\"red\")\n",
        "plt.title(\"PyTorch CNN\")\n",
        "plt.xlabel('Num. Epochs')\n",
        "plt.ylabel('Accuracy (blue=train, red=test)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mniCascdNl1d"
      },
      "source": [
        "EXPERIMENT 6 - ResNet Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "scYmiAyFamdR"
      },
      "outputs": [],
      "source": [
        "train_x, train_y, test_x, test_y = getData()\n",
        "print(\"Loaded data successfully.\")\n",
        "train_data = [(np.reshape(train_x[i], (3,32,32)).get(),torch.from_numpy(train_y[i].get()).to('cuda:0')) for i in range(len(train_x))]\n",
        "test_data = [(np.reshape(test_x[i], (3,32,32)).get(),torch.from_numpy(test_y[i].get()).to('cuda:0')) for i in range(len(test_x))]\n",
        "\n",
        "training_loader = torch.utils.data.DataLoader(train_data, batch_size=500, shuffle=False)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=500, shuffle=True)\n",
        "\n",
        "fc =  nn.Sequential(\n",
        "    nn.Linear(512, 512),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(512, 512),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(512, 10),\n",
        "    nn.Softmax(dim=1))\n",
        "\n",
        "model = CustomResnet18(fc)\n",
        "model.to(torch.device('cuda:0'))\n",
        "loss_fn = torch.nn.CrossEntropyLoss() \n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.5)\n",
        "\n",
        "def train_one_epoch(epoch_i):\n",
        "    for i, data in enumerate(training_loader):\n",
        "        inputs, labels = data\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = loss_fn(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    test_acc = torch_evaluate_acc(test_loader, model)\n",
        "    train_acc = torch_evaluate_acc(training_loader, model)\n",
        "    print(\"epoch\", epoch_i, \"completed. Train accuracy:\", train_acc, \". Test accuracy:\", test_acc)\n",
        "    return test_acc, train_acc\n",
        "\n",
        "\n",
        "train_accuracy_per_epoch = []\n",
        "test_accuracy_per_epoch = []\n",
        "epoch_num = 0\n",
        "last_test_acc_increase = 0\n",
        "best_test_acc = 0\n",
        "while epoch_num - last_test_acc_increase < 100:\n",
        "    test_acc, train_acc = train_one_epoch(epoch_num)\n",
        "    test_accuracy_per_epoch.append(test_acc)\n",
        "    train_accuracy_per_epoch.append(train_acc)\n",
        "    if test_acc > best_test_acc: last_test_acc_increase = epoch_num\n",
        "    best_test_acc = max(test_acc, best_test_acc)\n",
        "    epoch_num += 1\n",
        "\n",
        "print(\"Max test accuracy is\", best_test_acc)\n",
        "\n",
        "plt.figure(0)\n",
        "plt.plot(train_accuracy_per_epoch, color=\"blue\")\n",
        "plt.plot(test_accuracy_per_epoch, color=\"red\")\n",
        "plt.title(\"Resnet18 Pre-Trained model with custom FC\")\n",
        "plt.xlabel('Num. Epochs')\n",
        "plt.ylabel('Accuracy (blue=train, red=test)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "EXPERIMENT 7 - Increasing depth of MLP"
      ],
      "metadata": {
        "id": "Bned-XaMLMAM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_x, train_y, test_x, test_y = getData()\n",
        "print(\"Loaded data successfully.\")\n",
        "\n",
        "optimizer = GradientDescent(learning_rate=.1, batch_size=500, momentum=0.5)\n",
        "model = MLP(activation=relu, activation_gradient=relu_gradient, hidden_layers=3, hidden_units=[256, 256, 256])\n",
        "model.fit(train_x, train_y, optimizer, test_x, test_y)\n",
        "\n",
        "plt.figure(0)\n",
        "plt.plot(optimizer.train_accuracy_per_epoch, color=\"blue\")\n",
        "plt.plot(optimizer.test_accuracy_per_epoch, color=\"red\")\n",
        "plt.title(\"3 Hidden Layers, with 256 units, ReLu activation\")\n",
        "plt.xlabel('Num. Epochs')\n",
        "plt.ylabel('Accuracy (blue=train, red=test)')\n",
        "plt.show()\n",
        "\n",
        "model = MLP(activation=relu, activation_gradient=relu_gradient, hidden_layers=4, hidden_units=[256, 256, 256, 256])\n",
        "model.fit(train_x, train_y, optimizer, test_x, test_y)\n",
        "\n",
        "plt.figure(1)\n",
        "plt.plot(optimizer.train_accuracy_per_epoch, color=\"blue\")\n",
        "plt.plot(optimizer.test_accuracy_per_epoch, color=\"red\")\n",
        "plt.title(\"4 hidden Layers with 256 units, ReLu activation\")\n",
        "plt.xlabel('Num. Epochs')\n",
        "plt.ylabel('Accuracy (blue=train, red=test)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qX27DuGoLPPa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "EXPERIMENT 8 - Varying the width of MLP"
      ],
      "metadata": {
        "id": "KiZXbCatLWTq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Smaller width \n",
        "train_x, train_y, test_x, test_y = getData()\n",
        "print(\"Loaded data successfully.\")\n",
        "\n",
        "optimizer = GradientDescent(learning_rate=.1, batch_size=500, momentum=0.5)\n",
        "model = MLP(activation=relu, activation_gradient=relu_gradient, hidden_layers=2, hidden_units=[150, 150])\n",
        "model.fit(train_x, train_y, optimizer, test_x, test_y)\n",
        "\n",
        "plt.figure(0)\n",
        "plt.plot(optimizer.train_accuracy_per_epoch, color=\"blue\")\n",
        "plt.plot(optimizer.test_accuracy_per_epoch, color=\"red\")\n",
        "plt.title(\"2 Hidden Layers, with 150 units, ReLu activation\")\n",
        "plt.xlabel('Num. Epochs')\n",
        "plt.ylabel('Accuracy (blue=train, red=test)')\n",
        "plt.show()\n",
        "\n",
        "model = MLP(activation=relu, activation_gradient=relu_gradient, hidden_layers=2, hidden_units=[100, 100])\n",
        "model.fit(train_x, train_y, optimizer, test_x, test_y)\n",
        "\n",
        "plt.figure(1)\n",
        "plt.plot(optimizer.train_accuracy_per_epoch, color=\"blue\")\n",
        "plt.plot(optimizer.test_accuracy_per_epoch, color=\"red\")\n",
        "plt.title(\"2 hidden Layers with 100 units, ReLu activation\")\n",
        "plt.xlabel('Num. Epochs')\n",
        "plt.ylabel('Accuracy (blue=train, red=test)')\n",
        "plt.show()\n",
        "\n",
        "#Bigger width \n",
        "train_x, train_y, test_x, test_y = getData()\n",
        "print(\"Loaded data successfully.\")\n",
        "\n",
        "optimizer = GradientDescent(learning_rate=.1, batch_size=500, momentum=0.5)\n",
        "model = MLP(activation=relu, activation_gradient=relu_gradient, hidden_layers=2, hidden_units=[500, 500])\n",
        "model.fit(train_x, train_y, optimizer, test_x, test_y)\n",
        "\n",
        "plt.figure(0)\n",
        "plt.plot(optimizer.train_accuracy_per_epoch, color=\"blue\")\n",
        "plt.plot(optimizer.test_accuracy_per_epoch, color=\"red\")\n",
        "plt.title(\"2 Hidden Layers, with 500 units, ReLu activation\")\n",
        "plt.xlabel('Num. Epochs')\n",
        "plt.ylabel('Accuracy (blue=train, red=test)')\n",
        "plt.show()\n",
        "\n",
        "model = MLP(activation=relu, activation_gradient=relu_gradient, hidden_layers=2, hidden_units=[700, 700])\n",
        "model.fit(train_x, train_y, optimizer, test_x, test_y)\n",
        "\n",
        "plt.figure(1)\n",
        "plt.plot(optimizer.train_accuracy_per_epoch, color=\"blue\")\n",
        "plt.plot(optimizer.test_accuracy_per_epoch, color=\"red\")\n",
        "plt.title(\"2 hidden Layers with 700 units, ReLu activation\")\n",
        "plt.xlabel('Num. Epochs')\n",
        "plt.ylabel('Accuracy (blue=train, red=test)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4fq2m-jfLYF-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "EXPERIMENT 9 - Incresed width and depth MLP"
      ],
      "metadata": {
        "id": "64yHosfmPAAw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_x, train_y, test_x, test_y = getData()\n",
        "print(\"Loaded data successfully.\")\n",
        "\n",
        "optimizer = GradientDescent(learning_rate=.1, batch_size=500, momentum=0.5)\n",
        "model = MLP(activation=relu, activation_gradient=relu_gradient, hidden_layers=3, hidden_units=[500, 500, 500])\n",
        "model.fit(train_x, train_y, optimizer, test_x, test_y)\n",
        "\n",
        "plt.figure(0)\n",
        "plt.plot(optimizer.train_accuracy_per_epoch, color=\"blue\")\n",
        "plt.plot(optimizer.test_accuracy_per_epoch, color=\"red\")\n",
        "plt.title(\"2 Hidden Layers, with 150 units, ReLu activation\")\n",
        "plt.xlabel('Num. Epochs')\n",
        "plt.ylabel('Accuracy (blue=train, red=test)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Cq-urTAUPCb0"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "Qjzhh9sH0uT9"
      ],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}